{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple convolutional neural network on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages\n",
    "Here we are using [Keras](https://github.com/keras-team/keras) with [TensorFlow](https://www.tensorflow.org/) backend to develop a **convolutional neural network (CNN)** to training the classification of images from the [CIFAR-10 dataset] (https://www.toronto.edu/~kriz/cifar.html). \n",
    "\n",
    "\n",
    "#### Introduction\n",
    "The dataset consists of airplanes, dogs, cats, and other objects. I'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import print_summary, to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Hyperparameters\n",
    "We will feed the CNN with the images as batches of 64 images in 100 epochs. The model will output the probabilities of 10 different categories (num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "model_name = 'keras_cifar10_model'\n",
    "save_dir = '/model' + model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explicitly define our MNIST image dimensions which will be used later to reshape our date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Downloading\n",
    "[CIFAR-10 dataset for Python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) will be downloaded from keras datasets, i.e. cifar10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in the dataset are converted into categorical matrix structure from 1-dim numpy array structure using <b>keras.utils.to_categorical(y, num_classes=None, dtype='float')</b> <br />\n",
    "(source: https://keras.io/utils/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Comment out the two lines below to remove the normalization\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the CNN Model\n",
    "I will build the most common neural network model architecture in the format of [CONV]-[MaxPooling]-...-[CONV]-[MaxPooling]-[Dense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(80))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 80)                163920    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                810       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 257,978\n",
      "Trainable params: 257,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization using Stochastic Gradient Descent algorithm\n",
    "SGD is used to optimize the weights on the backpropagation. Momentum parameter is set as 0.9 and other parameters are left as default. <br />\n",
    "(Ref: https://leon.bottou.org/publications/pdf/compstat-2010.pdf, https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=learning_rate, momentum=Parameter that accelerates SGD in the relevant direction and dampens oscillations\n",
    "# decay=learning rate decay, nesterov= Whether to apply Nesterov momentum.\n",
    "opt = SGD(lr=0.01, momentum=0.9, decay=0, nesterov=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "I split the training dataset (50000 images) into training (40000 images) and validation (10000 images) datasets to measure the validation accuracy of our model. Thus, the neural network model will continue the training by evaluating the images that are not been during the training after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 2.0050 - acc: 0.2549 - val_loss: 1.6429 - val_acc: 0.4002\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 1.6184 - acc: 0.4019 - val_loss: 1.4370 - val_acc: 0.4691\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.4666 - acc: 0.4654 - val_loss: 1.3169 - val_acc: 0.5249\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 1.3709 - acc: 0.4971 - val_loss: 1.2686 - val_acc: 0.5401\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.2861 - acc: 0.5372 - val_loss: 1.1296 - val_acc: 0.5971\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 1.2257 - acc: 0.5598 - val_loss: 1.1114 - val_acc: 0.6027\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 1.1695 - acc: 0.5822 - val_loss: 1.0896 - val_acc: 0.6030\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 1.1249 - acc: 0.5972 - val_loss: 0.9992 - val_acc: 0.6536\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 1.0867 - acc: 0.6142 - val_loss: 0.9623 - val_acc: 0.6547\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 1.0594 - acc: 0.6203 - val_loss: 0.9532 - val_acc: 0.6609\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 76s 2ms/step - loss: 1.0286 - acc: 0.6354 - val_loss: 0.8909 - val_acc: 0.6898\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 1.0080 - acc: 0.6412 - val_loss: 0.9051 - val_acc: 0.6783\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.9911 - acc: 0.6471 - val_loss: 0.8359 - val_acc: 0.7086\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 76s 2ms/step - loss: 0.9711 - acc: 0.6542 - val_loss: 0.8477 - val_acc: 0.7062\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.9496 - acc: 0.6636 - val_loss: 0.9651 - val_acc: 0.6658\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.9468 - acc: 0.6678 - val_loss: 0.8155 - val_acc: 0.7153\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 0.9267 - acc: 0.6731 - val_loss: 0.8313 - val_acc: 0.7046\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.9093 - acc: 0.6797 - val_loss: 0.8410 - val_acc: 0.7056\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 0.8997 - acc: 0.6811 - val_loss: 0.7860 - val_acc: 0.7257\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 76s 2ms/step - loss: 0.8859 - acc: 0.6848 - val_loss: 0.7652 - val_acc: 0.7341\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 91s 2ms/step - loss: 0.8848 - acc: 0.6876 - val_loss: 0.8092 - val_acc: 0.7170\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 0.8658 - acc: 0.6941 - val_loss: 0.8562 - val_acc: 0.7023\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.8642 - acc: 0.6954 - val_loss: 0.7930 - val_acc: 0.7241\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 0.8536 - acc: 0.6977 - val_loss: 0.7785 - val_acc: 0.7338\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.8431 - acc: 0.7031 - val_loss: 0.7950 - val_acc: 0.7218\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.8382 - acc: 0.7054 - val_loss: 0.7955 - val_acc: 0.7234\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.8334 - acc: 0.7066 - val_loss: 0.7477 - val_acc: 0.7391\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 76s 2ms/step - loss: 0.8231 - acc: 0.7088 - val_loss: 0.9200 - val_acc: 0.6845\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.8137 - acc: 0.7096 - val_loss: 0.7397 - val_acc: 0.7413\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 86s 2ms/step - loss: 0.8116 - acc: 0.7134 - val_loss: 0.7155 - val_acc: 0.7510\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 0.8074 - acc: 0.7135 - val_loss: 0.7479 - val_acc: 0.7400\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 83s 2ms/step - loss: 0.7984 - acc: 0.7173 - val_loss: 0.7366 - val_acc: 0.7470\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 86s 2ms/step - loss: 0.7964 - acc: 0.7194 - val_loss: 0.7812 - val_acc: 0.7313\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 86s 2ms/step - loss: 0.7969 - acc: 0.7181 - val_loss: 0.6937 - val_acc: 0.7618\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.7894 - acc: 0.7207 - val_loss: 0.6926 - val_acc: 0.7644\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.7818 - acc: 0.7232 - val_loss: 0.7668 - val_acc: 0.7364\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 0.7828 - acc: 0.7244 - val_loss: 0.7276 - val_acc: 0.7485\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7791 - acc: 0.7260 - val_loss: 0.7239 - val_acc: 0.7471\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7681 - acc: 0.7297 - val_loss: 0.6862 - val_acc: 0.7634\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7726 - acc: 0.7291 - val_loss: 0.7240 - val_acc: 0.7477\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.7625 - acc: 0.7325 - val_loss: 0.7094 - val_acc: 0.7610\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7470 - acc: 0.7355 - val_loss: 0.7522 - val_acc: 0.7437\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.7593 - acc: 0.7320 - val_loss: 0.7017 - val_acc: 0.7593\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.7543 - acc: 0.7360 - val_loss: 0.7338 - val_acc: 0.7499\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.7491 - acc: 0.7339 - val_loss: 0.6686 - val_acc: 0.7710\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.7491 - acc: 0.7351 - val_loss: 0.6722 - val_acc: 0.7731\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 35367s 884ms/step - loss: 0.7481 - acc: 0.7359 - val_loss: 0.6791 - val_acc: 0.7645\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.7392 - acc: 0.7386 - val_loss: 0.7270 - val_acc: 0.7528\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 0.7367 - acc: 0.7403 - val_loss: 0.6653 - val_acc: 0.7749\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.7332 - acc: 0.7426 - val_loss: 0.6650 - val_acc: 0.7663\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 60s 2ms/step - loss: 0.7297 - acc: 0.7433 - val_loss: 0.6526 - val_acc: 0.7744\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 61s 2ms/step - loss: 0.7309 - acc: 0.7412 - val_loss: 0.6782 - val_acc: 0.7645\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 0.7301 - acc: 0.7405 - val_loss: 0.6835 - val_acc: 0.7630\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 55s 1ms/step - loss: 0.7268 - acc: 0.7441 - val_loss: 0.6660 - val_acc: 0.7685\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 0.7181 - acc: 0.7480 - val_loss: 0.7361 - val_acc: 0.7475\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 55s 1ms/step - loss: 0.7193 - acc: 0.7453 - val_loss: 0.6722 - val_acc: 0.7678\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 59s 1ms/step - loss: 0.7105 - acc: 0.7484 - val_loss: 0.6896 - val_acc: 0.7654\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.7210 - acc: 0.7473 - val_loss: 0.6836 - val_acc: 0.7668\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.7096 - acc: 0.7489 - val_loss: 0.6835 - val_acc: 0.7658\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.7163 - acc: 0.7486 - val_loss: 0.7127 - val_acc: 0.7653\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7033 - acc: 0.7505 - val_loss: 0.7362 - val_acc: 0.7501\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.7090 - acc: 0.7500 - val_loss: 0.6366 - val_acc: 0.7824\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.7017 - acc: 0.7541 - val_loss: 0.6567 - val_acc: 0.7743\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.7103 - acc: 0.7488 - val_loss: 0.6536 - val_acc: 0.7751\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.7035 - acc: 0.7531 - val_loss: 0.6548 - val_acc: 0.7748\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 0.6963 - acc: 0.7552 - val_loss: 0.6971 - val_acc: 0.7609\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7124 - acc: 0.7485 - val_loss: 0.6964 - val_acc: 0.7623\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.6972 - acc: 0.7561 - val_loss: 0.6921 - val_acc: 0.7673\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.6976 - acc: 0.7531 - val_loss: 0.6668 - val_acc: 0.7707\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.6928 - acc: 0.7558 - val_loss: 0.6413 - val_acc: 0.7784\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 0.6935 - acc: 0.7561 - val_loss: 0.7047 - val_acc: 0.7637\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 0.6956 - acc: 0.7520 - val_loss: 0.7054 - val_acc: 0.7624\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 79s 2ms/step - loss: 0.6865 - acc: 0.7576 - val_loss: 0.6601 - val_acc: 0.7760\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 91s 2ms/step - loss: 0.6852 - acc: 0.7588 - val_loss: 0.6515 - val_acc: 0.7764\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 0.6826 - acc: 0.7588 - val_loss: 0.6808 - val_acc: 0.7700\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 89s 2ms/step - loss: 0.6880 - acc: 0.7552 - val_loss: 0.7243 - val_acc: 0.7601\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 0.6868 - acc: 0.7589 - val_loss: 0.6923 - val_acc: 0.7632\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 0.6836 - acc: 0.7596 - val_loss: 0.6414 - val_acc: 0.7787\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 0.6854 - acc: 0.7575 - val_loss: 0.6378 - val_acc: 0.7818\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 88s 2ms/step - loss: 0.6773 - acc: 0.7636 - val_loss: 0.6531 - val_acc: 0.7756\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 0.6809 - acc: 0.7595 - val_loss: 0.7102 - val_acc: 0.7592\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 89s 2ms/step - loss: 0.6863 - acc: 0.7597 - val_loss: 0.6864 - val_acc: 0.77090.6866 - acc: 0.759\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 91s 2ms/step - loss: 0.6798 - acc: 0.7617 - val_loss: 0.6749 - val_acc: 0.7712\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 77s 2ms/step - loss: 0.6839 - acc: 0.7611 - val_loss: 0.6822 - val_acc: 0.7707\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 82s 2ms/step - loss: 0.6835 - acc: 0.7589 - val_loss: 0.6290 - val_acc: 0.7888\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 82s 2ms/step - loss: 0.6723 - acc: 0.7606 - val_loss: 0.6382 - val_acc: 0.7822\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 0.6682 - acc: 0.7654 - val_loss: 0.6713 - val_acc: 0.7729: 0.6683 - acc: 0.7\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6705 - acc: 0.7636 - val_loss: 0.6503 - val_acc: 0.7796\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 110s 3ms/step - loss: 0.6697 - acc: 0.7644 - val_loss: 0.6279 - val_acc: 0.7861\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 0.6698 - acc: 0.7635 - val_loss: 0.7506 - val_acc: 0.7401\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 86s 2ms/step - loss: 0.6684 - acc: 0.7624 - val_loss: 0.6935 - val_acc: 0.7647\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 79s 2ms/step - loss: 0.6793 - acc: 0.7611 - val_loss: 0.6303 - val_acc: 0.7849\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 77s 2ms/step - loss: 0.6667 - acc: 0.7654 - val_loss: 0.6622 - val_acc: 0.7760\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.6661 - acc: 0.7640 - val_loss: 0.6505 - val_acc: 0.7775\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.6734 - acc: 0.7624 - val_loss: 0.6904 - val_acc: 0.7683\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 0.6660 - acc: 0.7659 - val_loss: 0.6397 - val_acc: 0.7838\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.6678 - acc: 0.7651 - val_loss: 0.6259 - val_acc: 0.7879\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 0.6676 - acc: 0.7637 - val_loss: 0.6393 - val_acc: 0.7814\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 0.6687 - acc: 0.7648 - val_loss: 0.6517 - val_acc: 0.7806\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 0.6578 - acc: 0.7689 - val_loss: 0.6225 - val_acc: 0.7853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15924aa3c50>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train model with epoch size 100: 42825.29988884926 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print ('Time taken to train model with epoch size {}: {} seconds'.format(epochs, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 995us/step\n",
      "Test loss: 0.6422443160057068\n",
      "Test accuracy: 0.7798\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "20 epochs - Accuracy: 68.70%, Loss: 0.89, Time Taken: 1183.97 seconds <br />\n",
    "50 epochs - Accuracy: 72.86%, Loss: 0.73, Time Taken: 26853.56 seconds <br />\n",
    "100 epochs - Accuracy: 77.98%, Loss: 0.64, Time Taken: 42825.30 seconds<br />\n",
    "<p>\n",
    "From the results, it is obvious that higher epochs leads to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying the wrongly classified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 8 8 ... 5 1 7]\n"
     ]
    }
   ],
   "source": [
    "output = model.predict_classes(x_test)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "ind = np.where(np.equal(output, y_test)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-76ccd4ff5950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0merr_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0merr_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merr_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "err_x = x_test[ind[0]]\n",
    "err_y = output[ind[0]]\n",
    "print (err_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_class = 3\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "for cls, cls_name in enumerate(classes):\n",
    "  idxs = np.where(cls == err_y)\n",
    "  idxs = np.random.choice(idxs[0], examples_per_class, replace=False)\n",
    "  for i, idx in enumerate(idxs):\n",
    "    plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n",
    "    plt.imshow(err_x[idx].astype('uint8'), cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "      plt.title(cls_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Works and Improvements\n",
    "<b> Changing Optimizer</b> - We can use other algorithm to optimize the weights and observe the results. <br />\n",
    "<b> Changing Learning Rate</b> - We can decrease/increase the learning rate and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
